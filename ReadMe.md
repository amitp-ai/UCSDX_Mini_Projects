# Mini-Projects That I Completed

### Basic Info:
* Most of the mini-projects are done on paperspace (using my Github account) and three on Databricks (for Spark/SQL related ones)
* For paperspace projects, log into my paperspace account (using my Github account)
* For Spark projects
	download project from: 
	(https://drive.google.com/drive/folders/1VoweKytaQW_rnoro5EyalsC3w6WTDvPp) 
	and copy into Databricks: 
	(https://community.cloud.databricks.com/?o=7526501889408240)

---

### Project Info:

1. **mec-3.4.1-use-apis-to-retrieve-data (API)**  
	"It’s time to hone your APIs skills by working on a mini-project"

2. **mec-5.3.10-data-wrangling-with-pandas (Pandas)**  
	"In this data wrangling mini-project, you’ll practice transformation and visualization techniques using pandas. Knowing how to use these techniques will be especially helpful if you work at a company that keeps the majority of their data in relational databases and flat files. This mini-project has been adopted from the Brandon Rhodes tutorial that you completed earlier in the subunit and will focus on movie data from IMDB."

3. **mec-5.4.4-json-based-data-exercise (Pandas)**  
	"This World Bank dataset from a school quality improvement project in Ethiopia is a good example of a real-life dataset that you’re likely to encounter as an AI/ML engineer. Sharpen your data wrangling skills by completing your mini-project so that you can then apply what you’ve learned to your capstone, which will occur later on in the course."

4. **mec-5.5.4-web-scraping (Web Scraping)**  
	"In this mini-project, you'll create your own dataset through web-scraping. As you'll see first hand, the datasets that you collect through web scraping are typically messy and unruly, and will require a lot of wrangling before you can run them through a model."

5. **sql_with_spark_5-6-6 (PySpark/SQL)**  
	"For this project, you will use Databricks and work through a series of exercises using Spark SQL. The purpose of this project is to help you familiarize yourself with the Spark SQL interface, which scales easily, making it great for working with huge datasets. However, because we don’t want you to have to worry about changing your SQL queries, the dataset you will be working with is not very large."

6. **data_wrangling_at_scale_with_spark_6-4-1 (PySpark)**  
	"For this project, you will continue using Databricks and work with real-world datasets from NASA HTTP logs. The purpose of this project is to become familiar with both structured and unstructured data, analyze large-scale data with Spark, practice more advanced wrangling and cleaning techniques, and try your hand at data transformation."

	This is a good project on Explore-Transform-Load (ETL) pipeline at scale using Pyspark. In particular, a large dataset containing over 3 million records of web server logs is downloaded (i.e. extracted/sourced) from NASA's website. Each record is a raw string that is transformed into 7 distinct columns/fields using Regular Expressions. The dataset is further transformed by dropping/imputing null values. Exploratory data analysis (EDA) is then performed on the transformed dataset. Finally, the transformed dataset is saved (i.e. loaded) in two different formats, csv and json, for future use. 
